"""
An√°lisis de Vinos usando PCA (√Ålgebra Lineal para IA)
==================================================

Este script implementa An√°lisis de Componentes Principales (PCA) paso a paso
para demostrar conceptos fundamentales de √°lgebra lineal en inteligencia artificial.

Autor: [Tu Nombre]
Fecha: [Fecha actual]
Curso: Matem√°ticas para IA - √Ålgebra Lineal
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_wine
from sklearn.preprocessing import StandardScaler
import os

# Configurar estilo de visualizaci√≥n
plt.style.use('default')
sns.set_palette("husl")

# Crear carpeta de resultados si no existe
os.makedirs('results', exist_ok=True)

def cargar_dataset():
    """
    Carga y prepara el dataset de vinos para an√°lisis PCA.
    
    Returns:
        pd.DataFrame: DataFrame con los datos de vinos
        np.ndarray: Matriz de caracter√≠sticas (X)
        np.ndarray: Vector de clases (y)
        list: Nombres de las caracter√≠sticas
    """
    print("üç∑ AN√ÅLISIS DE CALIDAD DEL VINO USANDO PCA")
    print("="*60)
    
    # Cargar el dataset real de vinos (UCI Repository)
    wine_data = load_wine()
    
    # Convertir a DataFrame para mejor manipulaci√≥n
    df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)
    df['target'] = wine_data.target
    df['wine_class'] = df['target'].map({
        0: 'Clase_0 (Cultivar_1)', 
        1: 'Clase_1 (Cultivar_2)', 
        2: 'Clase_2 (Cultivar_3)'
    })
    
    # Informaci√≥n b√°sica del dataset
    print("üìã INFORMACI√ìN DEL DATASET:")
    print(f"‚Ä¢ N√∫mero de vinos analizados: {df.shape[0]}")
    print(f"‚Ä¢ N√∫mero de caracter√≠sticas qu√≠micas: {df.shape[1]-2}")
    print(f"‚Ä¢ Clases de vino: {list(wine_data.target_names)}")
    print(f"‚Ä¢ Distribuci√≥n de clases:")
    for i, name in enumerate(wine_data.target_names):
        count = sum(df.target == i)
        print(f"  - {name}: {count} muestras")
    
    print("\nüìä PRIMERAS 5 MUESTRAS:")
    print(df.head().round(2))
    
    print("\nüìà DIMENSIONES DEL PROBLEMA:")
    print(f"‚Ä¢ Datos de entrada: matriz de {df.shape[0]} √ó {df.shape[1]-2}")
    print(f"‚Ä¢ Sin PCA: necesitamos analizar {df.shape[1]-2} variables qu√≠micas")
    print(f"‚Ä¢ Con PCA: reduciremos a 2-3 componentes principales")
    
    # Preparar datos para el an√°lisis
    X = df.drop(['target', 'wine_class'], axis=1).values
    y = df['target'].values
    feature_names = df.drop(['target', 'wine_class'], axis=1).columns.tolist()
    
    print(f"\n‚úÖ Datos preparados:")
    print(f"‚Ä¢ X (caracter√≠sticas): {X.shape}")
    print(f"‚Ä¢ y (clases): {y.shape}")
    
    # Mostrar las variables que vamos a analizar
    print(f"\nüß™ VARIABLES QU√çMICAS A ANALIZAR:")
    for i, feature in enumerate(feature_names, 1):
        print(f"{i:2d}. {feature}")
    
    return df, X, y, feature_names

def analizar_estadisticas(df, X, feature_names):
    """
    Analiza las estad√≠sticas descriptivas y detecta la necesidad de estandarizaci√≥n.
    
    Args:
        df: DataFrame con los datos
        X: Matriz de caracter√≠sticas  
        feature_names: Nombres de las caracter√≠sticas
    """
    print("\n" + "="*60)
    print("üìä AN√ÅLISIS ESTAD√çSTICO PRE-PCA")
    print("="*60)
    
    # Estad√≠sticas descriptivas
    stats = pd.DataFrame({
        'Media': np.mean(X, axis=0),
        'Desv_Std': np.std(X, axis=0),
        'M√≠nimo': np.min(X, axis=0),
        'M√°ximo': np.max(X, axis=0),
        'Rango': np.max(X, axis=0) - np.min(X, axis=0)
    }, index=feature_names)
    
    print("üìà ESTAD√çSTICAS DESCRIPTIVAS:")
    print(stats.round(2))
    
    # Detectar variables con escalas muy diferentes
    print(f"\nüîç AN√ÅLISIS DE ESCALAS:")
    rangos = stats['Rango'].values
    max_rango = np.max(rangos)
    min_rango = np.min(rangos)
    ratio_escalas = max_rango / min_rango
    
    print(f"‚Ä¢ Variable con mayor rango: {feature_names[np.argmax(rangos)]} ({max_rango:.2f})")
    print(f"‚Ä¢ Variable con menor rango: {feature_names[np.argmin(rangos)]} ({min_rango:.2f})")
    print(f"‚Ä¢ Ratio de escalas: {ratio_escalas:.2f}")
    
    if ratio_escalas > 100:
        print("‚ö†Ô∏è  ADVERTENCIA: Las variables tienen escalas MUY diferentes")
        print("   Es CR√çTICO estandarizar antes de aplicar PCA")
    elif ratio_escalas > 10:
        print("‚ö° RECOMENDACI√ìN: Se recomienda estandarizar antes de PCA")
    else:
        print("‚úÖ Las escalas son relativamente similares")
    
    # Visualizar distribuciones
    crear_visualizacion_escalas(X, feature_names)
    
    return stats

def crear_visualizacion_escalas(X, feature_names):
    """
    Crea visualizaci√≥n para mostrar las diferentes escalas de las variables.
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Gr√°fico de cajas (boxplot) para ver escalas
    ax1.boxplot(X, labels=range(1, len(feature_names)+1))
    ax1.set_title('üìä Distribuci√≥n de Variables (Escalas Originales)')
    ax1.set_xlabel('Variable #')
    ax1.set_ylabel('Valor')
    ax1.tick_params(axis='x', rotation=45)
    
    # Gr√°fico de rangos
    rangos = np.max(X, axis=0) - np.min(X, axis=0)
    ax2.bar(range(len(feature_names)), rangos)
    ax2.set_title('üìè Rango de Cada Variable')
    ax2.set_xlabel('Variable #')
    ax2.set_ylabel('Rango (M√°x - M√≠n)')
    ax2.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('results/01_analisis_escalas.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"üíæ Gr√°fico guardado en: results/01_analisis_escalas.png")

def estandarizar_datos(X, feature_names):
    """
    Estandariza los datos para que todas las variables tengan media=0 y std=1.
    
    Args:
        X: Matriz de caracter√≠sticas original
        feature_names: Nombres de las caracter√≠sticas
        
    Returns:
        np.ndarray: Matriz estandarizada
        StandardScaler: Objeto scaler para transformaciones futuras
    """
    print("\n" + "="*60)
    print("‚öñÔ∏è  PASO 2: ESTANDARIZACI√ìN DE DATOS")
    print("="*60)
    
    print("üéØ OBJETIVO: Transformar todas las variables a la misma escala")
    print("   F√≥rmula: X_estandarizado = (X - media) / desviaci√≥n_est√°ndar")
    
    # Crear objeto estandarizador
    scaler = StandardScaler()
    
    # Calcular estad√≠sticas antes de estandarizar
    print("\nüìä ANTES DE ESTANDARIZAR:")
    print(f"‚Ä¢ Media m√≠nima: {np.min(np.mean(X, axis=0)):.2f}")
    print(f"‚Ä¢ Media m√°xima: {np.max(np.mean(X, axis=0)):.2f}")
    print(f"‚Ä¢ Std m√≠nima: {np.min(np.std(X, axis=0)):.2f}")
    print(f"‚Ä¢ Std m√°xima: {np.max(np.std(X, axis=0)):.2f}")
    
    # Estandarizar los datos
    X_scaled = scaler.fit_transform(X)
    
    # Verificar estandarizaci√≥n
    print("\nüìä DESPU√âS DE ESTANDARIZAR:")
    print(f"‚Ä¢ Media m√≠nima: {np.min(np.mean(X_scaled, axis=0)):.10f}")
    print(f"‚Ä¢ Media m√°xima: {np.max(np.mean(X_scaled, axis=0)):.10f}")
    print(f"‚Ä¢ Std m√≠nima: {np.min(np.std(X_scaled, axis=0)):.10f}")
    print(f"‚Ä¢ Std m√°xima: {np.max(np.std(X_scaled, axis=0)):.10f}")
    
    print("\n‚úÖ VERIFICACI√ìN: Todas las medias ‚âà 0 y todas las std ‚âà 1")
    
    # Crear visualizaci√≥n de la estandarizaci√≥n
    crear_visualizacion_estandarizacion(X, X_scaled, feature_names)
    
    return X_scaled, scaler

def crear_visualizacion_estandarizacion(X_original, X_scaled, feature_names):
    """
    Visualiza el efecto de la estandarizaci√≥n en los datos.
    """
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Datos originales
    ax1.boxplot(X_original, labels=range(1, len(feature_names)+1))
    ax1.set_title('üìä Datos Originales (Escalas Diferentes)')
    ax1.set_xlabel('Variable #')
    ax1.set_ylabel('Valor Original')
    ax1.tick_params(axis='x', rotation=45)
    
    # Datos estandarizados
    ax2.boxplot(X_scaled, labels=range(1, len(feature_names)+1))
    ax2.set_title('‚öñÔ∏è  Datos Estandarizados (Media=0, Std=1)')
    ax2.set_xlabel('Variable #')
    ax2.set_ylabel('Valor Estandarizado')
    ax2.tick_params(axis='x', rotation=45)
    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Media = 0')
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('results/02_estandarizacion.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"üíæ Gr√°fico guardado en: results/02_estandarizacion.png")

def calcular_matriz_covarianza(X_scaled, feature_names):
    """
    Calcula y analiza la matriz de covarianza paso a paso.
    
    Args:
        X_scaled: Matriz de datos estandarizados
        feature_names: Nombres de las caracter√≠sticas
        
    Returns:
        np.ndarray: Matriz de covarianza
    """
    print("\n" + "="*60)
    print("üìà PASO 3: MATRIZ DE COVARIANZA")
    print("="*60)
    
    print("üéØ OBJETIVO: Medir c√≥mo se relacionan las variables entre s√≠")
    print("   F√≥rmula: Cov = (X^T √ó X) / (n-1)")
    
    n_samples, n_features = X_scaled.shape
    print(f"\nüìê DIMENSIONES:")
    print(f"‚Ä¢ X_scaled: {n_samples} √ó {n_features}")
    print(f"‚Ä¢ X_scaled^T: {n_features} √ó {n_samples}")
    print(f"‚Ä¢ Covarianza: {n_features} √ó {n_features}")
    
    # Calcular matriz de covarianza manualmente
    print("\nüßÆ C√ÅLCULO PASO A PASO:")
    print("1Ô∏è‚É£  Transponer la matriz: X^T")
    X_T = X_scaled.T
    print(f"   X^T shape: {X_T.shape}")
    
    print("2Ô∏è‚É£  Multiplicar: X^T √ó X")
    XTX = X_T @ X_scaled
    print(f"   X^T √ó X shape: {XTX.shape}")
    
    print("3Ô∏è‚É£  Dividir por (n-1):")
    cov_matrix = XTX / (n_samples - 1)
    print(f"   Matriz de covarianza final: {cov_matrix.shape}")
    
    # Verificar con NumPy
    cov_numpy = np.cov(X_scaled.T)
    diferencia = np.max(np.abs(cov_matrix - cov_numpy))
    print(f"\n‚úÖ VERIFICACI√ìN con np.cov(): diferencia m√°xima = {diferencia:.10f}")
    
    # Analizar la matriz de covarianza
    print(f"\nüìä AN√ÅLISIS DE LA MATRIZ DE COVARIANZA:")
    diagonal = np.diag(cov_matrix)
    print(f"‚Ä¢ Elementos diagonales (varianzas): min={np.min(diagonal):.3f}, max={np.max(diagonal):.3f}")
    
    # Encontrar correlaciones m√°s fuertes
    cov_abs = np.abs(cov_matrix)
    np.fill_diagonal(cov_abs, 0)  # Ignorar diagonal
    max_idx = np.unravel_index(np.argmax(cov_abs), cov_abs.shape)
    print(f"‚Ä¢ Correlaci√≥n m√°s fuerte: {feature_names[max_idx[0]]} ‚Üî {feature_names[max_idx[1]]}")
    print(f"  Valor: {cov_matrix[max_idx]:.3f}")
    
    # Visualizar matriz de covarianza
    crear_visualizacion_covarianza(cov_matrix, feature_names)
    
    return cov_matrix

def crear_visualizacion_covarianza(cov_matrix, feature_names):
    """
    Crea un heatmap de la matriz de covarianza.
    """
    plt.figure(figsize=(12, 10))
    
    # Crear heatmap
    mask = np.triu(np.ones_like(cov_matrix, dtype=bool), k=1)
    sns.heatmap(cov_matrix, 
                annot=True, 
                fmt='.2f',
                cmap='RdBu_r',
                center=0,
                square=True,
                mask=mask,
                xticklabels=[f'V{i+1}' for i in range(len(feature_names))],
                yticklabels=[f'V{i+1}' for i in range(len(feature_names))],
                cbar_kws={'label': 'Covarianza'})
    
    plt.title('üìà Matriz de Covarianza\n(Solo tri√°ngulo inferior - sim√©trica)', fontsize=14)
    plt.xlabel('Variables')
    plt.ylabel('Variables')
    
    # A√±adir leyenda de variables
    legend_text = '\n'.join([f'V{i+1}: {name[:20]}...' if len(name) > 20 else f'V{i+1}: {name}' 
                            for i, name in enumerate(feature_names)])
    plt.figtext(1.02, 0.5, legend_text, fontsize=9, va='center')
    
    plt.tight_layout()
    plt.savefig('results/03_matriz_covarianza.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"üíæ Gr√°fico guardado en: results/03_matriz_covarianza.png")

def calcular_eigenvalues_eigenvectors(cov_matrix, feature_names):
    """
    Calcula eigenvalues y eigenvectors de la matriz de covarianza paso a paso.
    
    Args:
        cov_matrix: Matriz de covarianza
        feature_names: Nombres de las caracter√≠sticas
        
    Returns:
        tuple: (eigenvalues, eigenvectors, indices_ordenados)
    """
    print("\n" + "="*60)
    print("üîÆ PASO 4: EIGENVALUES Y EIGENVECTORS")
    print("="*60)
    
    print("üéØ OBJETIVO: Encontrar las direcciones de m√°xima varianza")
    print("   Ecuaci√≥n: Cov √ó v = Œª √ó v")
    print("   Donde: v = eigenvector, Œª = eigenvalue")
    
    # Calcular eigenvalues y eigenvectors
    print("\nüßÆ C√ÅLCULO:")
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
    
    print(f"‚Ä¢ Eigenvalues calculados: {len(eigenvalues)}")
    print(f"‚Ä¢ Eigenvectors shape: {eigenvectors.shape}")
    
    # Ordenar por eigenvalues (de mayor a menor)
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues_sorted = eigenvalues[idx]
    eigenvectors_sorted = eigenvectors[:, idx]
    
    print(f"\nüìä EIGENVALUES (Ordenados de mayor a menor):")
    cumulative_var = np.cumsum(eigenvalues_sorted) / np.sum(eigenvalues_sorted) * 100
    
    for i in range(min(5, len(eigenvalues_sorted))):  # Mostrar solo los primeros 5
        print(f"PC{i+1}: Œª = {eigenvalues_sorted[i]:.3f} "
              f"({eigenvalues_sorted[i]/np.sum(eigenvalues_sorted)*100:.1f}% varianza) "
              f"[Acumulada: {cumulative_var[i]:.1f}%]")
    
    # Analizar cu√°ntos componentes necesitamos
    var_80 = np.where(cumulative_var >= 80)[0][0] + 1
    var_90 = np.where(cumulative_var >= 90)[0][0] + 1
    var_95 = np.where(cumulative_var >= 95)[0][0] + 1
    
    print(f"\nüìà AN√ÅLISIS DE VARIANZA EXPLICADA:")
    print(f"‚Ä¢ Para capturar 80% de la informaci√≥n: {var_80} componentes")
    print(f"‚Ä¢ Para capturar 90% de la informaci√≥n: {var_90} componentes")
    print(f"‚Ä¢ Para capturar 95% de la informaci√≥n: {var_95} componentes")
    
    # Verificar propiedades matem√°ticas
    verificar_propiedades_eigen(cov_matrix, eigenvalues_sorted, eigenvectors_sorted)
    
    # Visualizar eigenvalues y eigenvectors
    crear_visualizacion_eigen(eigenvalues_sorted, eigenvectors_sorted, feature_names, cumulative_var)
    
    return eigenvalues_sorted, eigenvectors_sorted, idx

def verificar_propiedades_eigen(cov_matrix, eigenvalues, eigenvectors):
    """
    Verifica las propiedades matem√°ticas de eigenvalues y eigenvectors.
    """
    print(f"\nüîç VERIFICACI√ìN MATEM√ÅTICA:")
    
    # 1. Verificar Cov @ v = Œª @ v para el primer eigenvector
    v1 = eigenvectors[:, 0]
    lambda1 = eigenvalues[0]
    
    left_side = cov_matrix @ v1
    right_side = lambda1 * v1
    error = np.max(np.abs(left_side - right_side))
    
    print(f"1Ô∏è‚É£  Ecuaci√≥n Cov√óv = Œª√óv: error m√°ximo = {error:.10f}")
    
    # 2. Verificar que eigenvectors son ortonormales
    dot_products = []
    for i in range(min(3, eigenvectors.shape[1])):
        for j in range(i+1, min(3, eigenvectors.shape[1])):
            dot_prod = np.dot(eigenvectors[:, i], eigenvectors[:, j])
            dot_products.append(abs(dot_prod))
    
    max_dot = max(dot_products) if dot_products else 0
    print(f"2Ô∏è‚É£  Ortogonalidad: producto punto m√°ximo entre eigenvectors = {max_dot:.10f}")
    
    # 3. Verificar normas unitarias
    norms = [np.linalg.norm(eigenvectors[:, i]) for i in range(min(3, eigenvectors.shape[1]))]
    max_norm_error = max([abs(norm - 1.0) for norm in norms])
    print(f"3Ô∏è‚É£  Normalizaci√≥n: error m√°ximo de norma = {max_norm_error:.10f}")
    
    # 4. Verificar que la suma de eigenvalues = traza de la matriz
    trace_cov = np.trace(cov_matrix)
    sum_eigenvals = np.sum(eigenvalues)
    trace_error = abs(trace_cov - sum_eigenvals)
    print(f"4Ô∏è‚É£  Traza: suma eigenvalues = {sum_eigenvals:.6f}, traza matriz = {trace_cov:.6f}")
    print(f"     Error = {trace_error:.10f}")
    
    print("‚úÖ Todas las propiedades matem√°ticas verificadas correctamente!")

def crear_visualizacion_eigen(eigenvalues, eigenvectors, feature_names, cumulative_var):
    """
    Crea visualizaciones de eigenvalues y eigenvectors.
    """
    fig = plt.figure(figsize=(18, 12))
    
    # 1. Gr√°fico de eigenvalues (scree plot)
    ax1 = plt.subplot(2, 3, 1)
    plt.plot(range(1, len(eigenvalues)+1), eigenvalues, 'bo-', linewidth=2, markersize=8)
    plt.title('üìä Scree Plot - Eigenvalues')
    plt.xlabel('Componente Principal')
    plt.ylabel('Eigenvalue (Varianza)')
    plt.grid(True, alpha=0.3)
    
    # 2. Varianza explicada acumulativa
    ax2 = plt.subplot(2, 3, 2)
    plt.plot(range(1, len(cumulative_var)+1), cumulative_var, 'ro-', linewidth=2, markersize=8)
    plt.axhline(y=80, color='green', linestyle='--', alpha=0.7, label='80%')
    plt.axhline(y=90, color='orange', linestyle='--', alpha=0.7, label='90%')
    plt.axhline(y=95, color='red', linestyle='--', alpha=0.7, label='95%')
    plt.title('üìà Varianza Explicada Acumulativa')
    plt.xlabel('N√∫mero de Componentes')
    plt.ylabel('% Varianza Acumulativa')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. Heatmap de los primeros 3 eigenvectors
    ax3 = plt.subplot(2, 3, 3)
    eigenvectors_subset = eigenvectors[:, :3]
    sns.heatmap(eigenvectors_subset.T, 
                annot=True, 
                fmt='.3f',
                cmap='RdBu_r',
                center=0,
                xticklabels=[f'V{i+1}' for i in range(len(feature_names))],
                yticklabels=['PC1', 'PC2', 'PC3'],
                cbar_kws={'label': 'Peso del eigenvector'})
    plt.title('üéØ Primeros 3 Eigenvectors\n(Pesos de cada variable)')
    
    # 4. Gr√°fico de barras para PC1
    ax4 = plt.subplot(2, 3, 4)
    colors = ['red' if x < 0 else 'blue' for x in eigenvectors[:, 0]]
    plt.barh(range(len(feature_names)), eigenvectors[:, 0], color=colors, alpha=0.7)
    plt.title('üîç Primer Componente Principal (PC1)')
    plt.xlabel('Peso en PC1')
    plt.yticks(range(len(feature_names)), [f'V{i+1}' for i in range(len(feature_names))])
    plt.grid(True, alpha=0.3)
    
    # 5. Gr√°fico de barras para PC2
    ax5 = plt.subplot(2, 3, 5)
    colors = ['red' if x < 0 else 'blue' for x in eigenvectors[:, 1]]
    plt.barh(range(len(feature_names)), eigenvectors[:, 1], color=colors, alpha=0.7)
    plt.title('üîç Segundo Componente Principal (PC2)')
    plt.xlabel('Peso en PC2')
    plt.yticks(range(len(feature_names)), [f'V{i+1}' for i in range(len(feature_names))])
    plt.grid(True, alpha=0.3)
    
    # 6. Contribuci√≥n de variables a PC1 y PC2
    ax6 = plt.subplot(2, 3, 6)
    pc1_contrib = eigenvectors[:, 0]**2
    pc2_contrib = eigenvectors[:, 1]**2
    
    x = np.arange(len(feature_names))
    width = 0.35
    
    plt.bar(x - width/2, pc1_contrib, width, label='PC1', alpha=0.7)
    plt.bar(x + width/2, pc2_contrib, width, label='PC2', alpha=0.7)
    plt.title('‚ö° Contribuci√≥n de Variables\n(Pesos al cuadrado)')
    plt.xlabel('Variables')
    plt.ylabel('Contribuci√≥n')
    plt.xticks(x, [f'V{i+1}' for i in range(len(feature_names))], rotation=45)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/04_eigenvalues_eigenvectors.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"üíæ Gr√°fico guardado en: results/04_eigenvalues_eigenvectors.png")

def realizar_pca_transformacion(X_scaled, eigenvectors, feature_names):
    """
    Realiza la transformaci√≥n PCA proyectando los datos en los nuevos ejes.
    
    Args:
        X_scaled: Datos estandarizados
        eigenvectors: Eigenvectors calculados
        feature_names: Nombres de las caracter√≠sticas
        
    Returns:
        np.ndarray: Datos transformados por PCA
    """
    print("\n" + "="*60)
    print("üîÑ PASO 5: TRANSFORMACI√ìN PCA")
    print("="*60)
    
    print("üéØ OBJETIVO: Proyectar datos en las nuevas componentes principales")
    print("   F√≥rmula: X_pca = X_scaled √ó Eigenvectors")
    
    # Realizar transformaci√≥n PCA
    print(f"\nüßÆ TRANSFORMACI√ìN:")
    print(f"‚Ä¢ X_scaled shape: {X_scaled.shape}")
    print(f"‚Ä¢ Eigenvectors shape: {eigenvectors.shape}")
    
    X_pca = X_scaled @ eigenvectors
    print(f"‚Ä¢ X_pca shape: {X_pca.shape}")
    
    # An√°lisis de la transformaci√≥n
    print(f"\nüìä AN√ÅLISIS DE LOS DATOS TRANSFORMADOS:")
    for i in range(min(3, X_pca.shape[1])):
        variance = np.var(X_pca[:, i])
        print(f"PC{i+1}: varianza = {variance:.3f}")
    
    # Verificar que las varianzas son los eigenvalues
    print(f"\n‚úÖ VERIFICACI√ìN:")
    print("Las varianzas de los componentes principales deben ser iguales a los eigenvalues")
    
    return X_pca

def visualizacion_final_2d(X_pca, y, feature_names, eigenvalues, eigenvectors):
    """
    Crea la visualizaci√≥n final en 2D y analiza los resultados.
    
    Args:
        X_pca: Datos transformados por PCA
        y: Clases de vino
        feature_names: Nombres de las caracter√≠sticas
        eigenvalues: Eigenvalues calculados
        eigenvectors: Eigenvectors calculados
    """
    print("\n" + "="*60)
    print("üé® PASO 6: VISUALIZACI√ìN FINAL Y AN√ÅLISIS")
    print("="*60)
    
    print("üéØ OBJETIVO: Visualizar 178 vinos en solo 2 dimensiones")
    print("   De 13D ‚Üí 2D manteniendo la m√°xima informaci√≥n posible")
    
    # Calcular informaci√≥n capturada
    total_variance = np.sum(eigenvalues)
    pc1_variance = eigenvalues[0] / total_variance * 100
    pc2_variance = eigenvalues[1] / total_variance * 100
    pc12_variance = (eigenvalues[0] + eigenvalues[1]) / total_variance * 100
    
    print(f"\nüìä INFORMACI√ìN CAPTURADA:")
    print(f"‚Ä¢ PC1: {pc1_variance:.1f}% de la varianza total")
    print(f"‚Ä¢ PC2: {pc2_variance:.1f}% de la varianza total")
    print(f"‚Ä¢ PC1 + PC2: {pc12_variance:.1f}% de la varianza total")
    print(f"‚Ä¢ Perdemos solo: {100 - pc12_variance:.1f}% de la informaci√≥n")
    
    # Crear visualizaci√≥n completa
    fig = plt.figure(figsize=(20, 15))
    
    # 1. Scatter plot principal en 2D
    ax1 = plt.subplot(2, 3, 1)
    colors = ['red', 'blue', 'green']
    wine_classes = ['Cultivar 1', 'Cultivar 2', 'Cultivar 3']
    
    for i, (color, wine_class) in enumerate(zip(colors, wine_classes)):
        mask = y == i
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                   c=color, label=wine_class, alpha=0.7, s=60)
    
    plt.xlabel(f'PC1 ({pc1_variance:.1f}% varianza)')
    plt.ylabel(f'PC2 ({pc2_variance:.1f}% varianza)')
    plt.title('üç∑ Vinos Proyectados en 2D\n(178 vinos de 13D ‚Üí 2D)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 2. An√°lisis de separaci√≥n de clases
    ax2 = plt.subplot(2, 3, 2)
    
    # Calcular centroides de cada clase
    centroids = []
    for i in range(3):
        mask = y == i
        centroid = [np.mean(X_pca[mask, 0]), np.mean(X_pca[mask, 1])]
        centroids.append(centroid)
        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], 
                   c=colors[i], alpha=0.3, s=40)
        plt.scatter(centroid[0], centroid[1], 
                   c=colors[i], s=200, marker='X', 
                   edgecolors='black', linewidth=2, label=f'Centro {wine_classes[i]}')
    
    plt.xlabel(f'PC1 ({pc1_variance:.1f}% varianza)')
    plt.ylabel(f'PC2 ({pc2_variance:.1f}% varianza)')
    plt.title('üéØ Centroides de Clases\n(Separaci√≥n autom√°tica)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 3. Biplot - Variables originales en el espacio PC
    ax3 = plt.subplot(2, 3, 3)
    
    # Proyectar las variables originales en el espacio PC
    scale_factor = 3
    for i, feature in enumerate(feature_names):
        arrow_x = eigenvectors[i, 0] * scale_factor
        arrow_y = eigenvectors[i, 1] * scale_factor
        plt.arrow(0, 0, arrow_x, arrow_y, 
                 head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.7)
        plt.text(arrow_x*1.1, arrow_y*1.1, f'V{i+1}', fontsize=8, ha='center')
    
    plt.xlabel(f'PC1 ({pc1_variance:.1f}% varianza)')
    plt.ylabel(f'PC2 ({pc2_variance:.1f}% varianza)')
    plt.title('üß≠ Biplot - Contribuci√≥n de Variables\n(Flechas = variables originales)')
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    
    # 4. Distribuci√≥n en PC1
    ax4 = plt.subplot(2, 3, 4)
    for i, (color, wine_class) in enumerate(zip(colors, wine_classes)):
        mask = y == i
        plt.hist(X_pca[mask, 0], alpha=0.6, color=color, 
                label=wine_class, bins=15, density=True)
    
    plt.xlabel(f'PC1 ({pc1_variance:.1f}% varianza)')
    plt.ylabel('Densidad')
    plt.title('üìä Distribuci√≥n en PC1')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 5. Distribuci√≥n en PC2
    ax5 = plt.subplot(2, 3, 5)
    for i, (color, wine_class) in enumerate(zip(colors, wine_classes)):
        mask = y == i
        plt.hist(X_pca[mask, 1], alpha=0.6, color=color, 
                label=wine_class, bins=15, density=True)
    
    plt.xlabel(f'PC2 ({pc2_variance:.1f}% varianza)')
    plt.ylabel('Densidad')
    plt.title('üìä Distribuci√≥n en PC2')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 6. Comparaci√≥n 3D vs 2D
    ax6 = plt.subplot(2, 3, 6)
    pc3_variance = eigenvalues[2] / total_variance * 100
    pc123_variance = (eigenvalues[0] + eigenvalues[1] + eigenvalues[2]) / total_variance * 100
    
    components = ['PC1', 'PC2', 'PC3', 'PC4-PC13']
    variances = [pc1_variance, pc2_variance, pc3_variance, 100-pc123_variance]
    colors_bar = ['darkblue', 'blue', 'lightblue', 'lightgray']
    
    bars = plt.bar(components, variances, color=colors_bar, alpha=0.8)
    plt.ylabel('% Varianza Explicada')
    plt.title('üìà Importancia de Componentes\n(¬øPor qu√© 2D es suficiente?)')
    plt.grid(True, alpha=0.3)
    
    # A√±adir valores en las barras
    for bar, variance in zip(bars, variances):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                f'{variance:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('results/05_visualizacion_final_2d.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"üíæ Gr√°fico guardado en: results/05_visualizacion_final_2d.png")
    
    # An√°lisis de los resultados
    analizar_resultados_pca(X_pca, y, centroids, pc12_variance)

def analizar_resultados_pca(X_pca, y, centroids, variance_explained):
    """
    Analiza los resultados del PCA y su interpretaci√≥n.
    """
    print(f"\nüîç AN√ÅLISIS DE RESULTADOS:")
    
    # Calcular distancias entre centroides
    dist_01 = np.sqrt((centroids[0][0] - centroids[1][0])**2 + (centroids[0][1] - centroids[1][1])**2)
    dist_02 = np.sqrt((centroids[0][0] - centroids[2][0])**2 + (centroids[0][1] - centroids[2][1])**2)
    dist_12 = np.sqrt((centroids[1][0] - centroids[2][0])**2 + (centroids[1][1] - centroids[2][1])**2)
    
    print(f"üìè SEPARACI√ìN ENTRE CLASES:")
    print(f"‚Ä¢ Cultivar 1 ‚Üî Cultivar 2: {dist_01:.2f} unidades")
    print(f"‚Ä¢ Cultivar 1 ‚Üî Cultivar 3: {dist_02:.2f} unidades")
    print(f"‚Ä¢ Cultivar 2 ‚Üî Cultivar 3: {dist_12:.2f} unidades")
    
    # Evaluar calidad de la separaci√≥n
    if min(dist_01, dist_02, dist_12) > 2:
        print("‚úÖ EXCELENTE separaci√≥n: Las clases est√°n bien diferenciadas")
    elif min(dist_01, dist_02, dist_12) > 1:
        print("‚úÖ BUENA separaci√≥n: Las clases son distinguibles")
    else:
        print("‚ö†Ô∏è  SEPARACI√ìN MODERADA: Algunas clases se superponen")
    
    print(f"\nüéØ LOGROS DEL PCA:")
    print(f"‚Ä¢ ‚úÖ Reducimos 13 dimensiones ‚Üí 2 dimensiones")
    print(f"‚Ä¢ ‚úÖ Mantenemos {variance_explained:.1f}% de la informaci√≥n")
    print(f"‚Ä¢ ‚úÖ Separamos autom√°ticamente las 3 clases de vino")
    print(f"‚Ä¢ ‚úÖ Podemos visualizar patrones antes invisibles")

def aplicaciones_en_ia(eigenvalues, feature_names):
    """
    Explica las aplicaciones del PCA en Inteligencia Artificial.
    """
    print("\n" + "="*60)
    print("ü§ñ APLICACIONES EN INTELIGENCIA ARTIFICIAL")
    print("="*60)
    
    print("üéØ LO QUE ACABAMOS DE HACER ES LA BASE DE:")
    
    print(f"\n1Ô∏è‚É£  üñºÔ∏è  COMPUTER VISION (Visi√≥n por Computador):")
    print(f"   ‚Ä¢ Reconocimiento facial: Eigenfaces (PCA en im√°genes de caras)")
    print(f"   ‚Ä¢ Compresi√≥n de im√°genes: Reducir p√≠xeles manteniendo informaci√≥n visual")
    print(f"   ‚Ä¢ Detecci√≥n de objetos: Reducir dimensionalidad antes de clasificar")
    
    print(f"\n2Ô∏è‚É£  üìä MACHINE LEARNING:")
    print(f"   ‚Ä¢ Preprocesamiento: Reducir overfitting con menos dimensiones")
    print(f"   ‚Ä¢ Visualizaci√≥n: Entender datos de alta dimensi√≥n")
    print(f"   ‚Ä¢ Feature selection: Identificar variables m√°s importantes")
    
    print(f"\n3Ô∏è‚É£  üó£Ô∏è  PROCESAMIENTO DE LENGUAJE NATURAL:")
    print(f"   ‚Ä¢ Word embeddings: Reducir vectores de palabras (Word2Vec ‚Üí PCA)")
    print(f"   ‚Ä¢ An√°lisis de sentimientos: Encontrar patrones en texto")
    print(f"   ‚Ä¢ Traducci√≥n autom√°tica: Mapear idiomas en espacios reducidos")
    
    print(f"\n4Ô∏è‚É£  üéµ SISTEMAS DE RECOMENDACI√ìN:")
    print(f"   ‚Ä¢ Netflix: Factorizaci√≥n matricial (similar a PCA)")
    print(f"   ‚Ä¢ Spotify: Reducir perfiles musicales de usuarios")
    print(f"   ‚Ä¢ Amazon: Recomendar productos basado en patrones latentes")
    
    print(f"\n5Ô∏è‚É£  üß† DEEP LEARNING:")
    print(f"   ‚Ä¢ Autoencoders: Redes neuronales que hacen PCA no-lineal")
    print(f"   ‚Ä¢ Initializaci√≥n: Usar PCA para inicializar pesos")
    print(f"   ‚Ä¢ Regularizaci√≥n: Proyectar en subespacios para evitar overfitting")
    
    print(f"\n6Ô∏è‚É£  üìà BIG DATA:")
    print(f"   ‚Ä¢ Reducir datasets masivos manteniendo informaci√≥n clave")
    print(f"   ‚Ä¢ Clustering eficiente en espacios de menor dimensi√≥n")
    print(f"   ‚Ä¢ An√°lisis exploratorio de datos complejos")
    
    # Crear resumen de conceptos de √°lgebra lineal aplicados
    print(f"\nüßÆ CONCEPTOS DE √ÅLGEBRA LINEAL QUE DOMINASTE:")
    print(f"   ‚úÖ Representaci√≥n matricial de datos")
    print(f"   ‚úÖ Transformaciones lineales (estandarizaci√≥n)")
    print(f"   ‚úÖ Producto matricial (matriz de covarianza)")
    print(f"   ‚úÖ Eigenvalues y eigenvectors (descomposici√≥n espectral)")
    print(f"   ‚úÖ Cambio de base y proyecciones")
    print(f"   ‚úÖ Reducci√≥n de dimensionalidad")
    print(f"   ‚úÖ Interpretaci√≥n geom√©trica del √°lgebra lineal")

def generar_informe_final(df, X, X_scaled, X_pca, y, feature_names, eigenvalues, eigenvectors, variance_explained):
    """
    Genera un informe final con todos los resultados.
    """
    print("\n" + "="*60)
    print("üìã INFORME FINAL - AN√ÅLISIS PCA DE VINOS")
    print("="*60)
    
    informe = f"""
AN√ÅLISIS DE COMPONENTES PRINCIPALES (PCA) - DATASET DE VINOS
===========================================================

DATOS ANALIZADOS:
‚Ä¢ Dataset: UCI Wine Dataset
‚Ä¢ Muestras: {df.shape[0]} vinos
‚Ä¢ Variables originales: {len(feature_names)} caracter√≠sticas qu√≠micas
‚Ä¢ Clases: 3 tipos de vino (Cultivar 1, 2, 3)

TRANSFORMACI√ìN REALIZADA:
‚Ä¢ Estandarizaci√≥n: Media = 0, Desviaci√≥n est√°ndar = 1
‚Ä¢ Dimensionalidad: {len(feature_names)}D ‚Üí 2D
‚Ä¢ Informaci√≥n conservada: {variance_explained:.1f}%
‚Ä¢ Informaci√≥n perdida: {100-variance_explained:.1f}%

COMPONENTES PRINCIPALES:
‚Ä¢ PC1: {eigenvalues[0]/np.sum(eigenvalues)*100:.1f}% de la varianza
‚Ä¢ PC2: {eigenvalues[1]/np.sum(eigenvalues)*100:.1f}% de la varianza
‚Ä¢ PC3: {eigenvalues[2]/np.sum(eigenvalues)*100:.1f}% de la varianza

RESULTADOS:
‚úÖ Separaci√≥n exitosa de las 3 clases de vino en 2D
‚úÖ Visualizaci√≥n clara de patrones antes invisibles
‚úÖ Reducci√≥n significativa de complejidad manteniendo informaci√≥n
‚úÖ Aplicaci√≥n exitosa de conceptos fundamentales de √°lgebra lineal

CONCEPTOS DE √ÅLGEBRA LINEAL APLICADOS:
1. Matrices y vectores para representaci√≥n de datos
2. Transformaciones lineales para estandarizaci√≥n
3. Producto matricial para matriz de covarianza
4. Eigenvalues y eigenvectors para encontrar direcciones principales
5. Cambio de base para proyecci√≥n en nuevos ejes
6. Reducci√≥n de dimensionalidad preservando m√°xima informaci√≥n

APLICACIONES EN IA:
‚Ä¢ Computer Vision: Eigenfaces, compresi√≥n de im√°genes
‚Ä¢ Machine Learning: Preprocesamiento, reducci√≥n de overfitting
‚Ä¢ NLP: Word embeddings, an√°lisis de sentimientos
‚Ä¢ Sistemas de recomendaci√≥n: Factorizaci√≥n matricial
‚Ä¢ Deep Learning: Autoencoders, inicializaci√≥n
‚Ä¢ Big Data: An√°lisis eficiente de datasets masivos
    """
    
    # Guardar informe
    with open('results/informe_final_pca.txt', 'w', encoding='utf-8') as f:
        f.write(informe)
    
    print(informe)
    print(f"üíæ Informe completo guardado en: results/informe_final_pca.txt")

def main():
    """
    Funci√≥n principal que ejecuta el an√°lisis paso a paso.
    """
    print("üöÄ INICIANDO AN√ÅLISIS DE VINOS CON PCA")
    print("Paso 1 de 6: Cargar y explorar dataset\n")
    
    # Paso 1: Cargar dataset
    df, X, y, feature_names = cargar_dataset()
    
    # Paso 2: An√°lisis estad√≠stico inicial
    stats = analizar_estadisticas(df, X, feature_names)
    
    # Paso 3: Estandarizaci√≥n
    X_scaled, scaler = estandarizar_datos(X, feature_names)
    
    # Paso 4: Matriz de covarianza
    cov_matrix = calcular_matriz_covarianza(X_scaled, feature_names)
    
    # Paso 5: Eigenvalues y eigenvectors
    eigenvalues, eigenvectors, idx = calcular_eigenvalues_eigenvectors(cov_matrix, feature_names)
    
    # Paso 6: Transformaci√≥n PCA
    X_pca = realizar_pca_transformacion(X_scaled, eigenvectors, feature_names)
    
    # Paso 7: Visualizaci√≥n final
    visualizacion_final_2d(X_pca, y, feature_names, eigenvalues, eigenvectors)
    
    # Paso 8: Aplicaciones en IA
    aplicaciones_en_ia(eigenvalues, feature_names)
    
    # Paso 9: Informe final
    pc12_variance = (eigenvalues[0] + eigenvalues[1]) / np.sum(eigenvalues) * 100
    generar_informe_final(df, X, X_scaled, X_pca, y, feature_names, eigenvalues, eigenvectors, pc12_variance)
    
    print("\n" + "="*60)
    print("üéâ AN√ÅLISIS PCA COMPLETADO EXITOSAMENTE")
    print("="*60)
    print("üí° Has aplicado conceptos fundamentales de √°lgebra lineal")
    print("ü§ñ Has visto aplicaciones directas en Inteligencia Artificial")
    print("üìä Has reducido 13 dimensiones a 2 manteniendo m√°xima informaci√≥n")
    print("üç∑ Has separado autom√°ticamente 3 tipos de vino")
    
    return df, X, X_scaled, X_pca, y, feature_names, stats, cov_matrix, eigenvalues, eigenvectors, scaler

if __name__ == "__main__":
    # Ejecutar an√°lisis completo
    results = main()
    df, X, X_scaled, X_pca, y, feature_names, stats, cov_matrix, eigenvalues, eigenvectors, scaler = results
    
    print(f"\nüéØ MISI√ìN CUMPLIDA - TODOS LOS PASOS COMPLETADOS")
    print(f"üìÅ Archivos generados en la carpeta 'results/':")
    print(f"   ‚Ä¢ 01_analisis_escalas.png")
    print(f"   ‚Ä¢ 02_estandarizacion.png") 
    print(f"   ‚Ä¢ 03_matriz_covarianza.png")
    print(f"   ‚Ä¢ 04_eigenvalues_eigenvectors.png")
    print(f"   ‚Ä¢ 05_visualizacion_final_2d.png")
    print(f"   ‚Ä¢ informe_final_pca.txt")
    print(f"\nüèÜ FELICITACIONES: Has completado un an√°lisis PCA profesional!")
    print(f"üìö Ahora entiendes c√≥mo el √°lgebra lineal potencia la IA")